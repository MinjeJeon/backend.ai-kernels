FROM nvcr.io/nvidia/pytorch:22.03-py3
# NVIDIA NGC PyTorch with Python 3.8 (CONDA)

ENV DEBIAN_FRONTEND=noninteractive \
    MPLBACKEND=Svg \
    PIP_IGNORE_INSTALLED=0 \
    PYTHONUNBUFFERED=1 \
    LD_LIBRARY_PATH="/usr/local/cuda/compat/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib:/usr/local/cuda/lib64:/usr/local/nvidia/lib64:/usr/local/cuda-11.6/include:/usr/include/x86_64-linux-gnu:$LD_LIBRARY_PATH" \
<<<<<<< HEAD
    PATH="/usr/local/nvm/versions/node/v16.6.1/bin:/opt/conda/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/cuda-11.6/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin:/opt/tensorrt/bin:/usr/local/src/lightgbm/LightGBM:/usr/local/bin/mecab:$PATH" \
    mecab_dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic \
    CPLUS_INCLUDE_PATH=/usr/include/gdal \
    C_INCLUDE_PATH=/usr/include/gdal \
    CPATH=/usr/local/cuda-11.6/targets/x86_64-linux/include:$CPATH \
=======
    PATH="/usr/local/nvm/versions/node/v16.6.1/bin:/opt/conda/lib/python3.8/site-packages/torch_tensorrt/bin:/opt/conda/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin:/opt/tensorrt/bin:/usr/local/src/lightgbm/LightGBM:/usr/local/bin/mecab" \
    mecab_dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic \
    CPLUS_INCLUDE_PATH=/usr/include/gdal \
    C_INCLUDE_PATH=/usr/include/gdal \
#    CPATH=/usr/local/cuda-10.1/targets/x86_64-linux/include:$CPATH \
>>>>>>> 428b8f2fe85b44c569e7749658a1f56e4b7778b3
    LANG=C.UTF-8

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
	automake \
<<<<<<< HEAD
=======
	bzip2 \
	cabextract \
>>>>>>> 428b8f2fe85b44c569e7749658a1f56e4b7778b3
	ffmpeg \
	fonts-nanum \
	fonts-nanum-coding \
	fonts-nanum-extra \
	gfortran \
	htop \
	libasound2-dev \
	libatlas-base-dev \
	libavresample-dev \
	libdc1394-22-dev \
	libeigen3-dev \
	libfaac-dev \
	libgdal-dev \
	libgflags-dev \
	libgoogle-glog-dev \
	libgphoto2-dev \
	libgstreamer-plugins-base1.0-dev \
	libgstreamer1.0-dev \
	libgtk-3-dev \
<<<<<<< HEAD
	libhdf5-dev \	
	libmp3lame-dev \
=======
	libhdf5-dev \
	libjemalloc-dev \
	libmp3lame-dev \
	libncurses-dev \
>>>>>>> 428b8f2fe85b44c569e7749658a1f56e4b7778b3
	libopenblas-dev \
	libopencore-amrnb-dev \
	libopencore-amrwb-dev \
	libprotobuf-dev \
	libtheora-dev \
	libvorbis-dev \
	libx264-dev \
	libxext6 \
	libxrender-dev \
	libxvidcore-dev \
	libsm6 \
	libtbb-dev \
	mercurial \
	ncurses-term \
<<<<<<< HEAD
    openjdk-8-jdk \
=======
        openjdk-8-jdk \
	pbzip2 \
	pv \
>>>>>>> 428b8f2fe85b44c569e7749658a1f56e4b7778b3
	pdsh \
	protobuf-compiler \
	v4l-utils \	
	x264 

# Install CUDA + cuDNN 
RUN ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.8.3.2 /usr/local/cuda/lib64/libcudnn.so && \
    rm -rf /var/lib/apt/lists/* && \
    ldconfig

# Install cmake
RUN wget https://github.com/Kitware/CMake/releases/download/v3.22.0/cmake-3.22.0-Linux-x86_64.sh \
         -q -O /tmp/cmake-install.sh && \
    chmod u+x /tmp/cmake-install.sh && \
    mkdir /usr/bin/cmake && \
    /tmp/cmake-install.sh --skip-license --prefix=/usr/bin/cmake && \
    rm /tmp/cmake-install.sh

# nvtop install
WORKDIR /tmp
RUN git clone https://github.com/Syllo/nvtop.git && \
    mkdir -p nvtop/build && \
    cd /tmp/nvtop/build && \
    cmake .. && \
    cmake .. -DNVML_RETRIEVE_HEADER_ONLINE=True && \
    make -j$(nproc) && \
    make install
    
RUN update-alternatives --install /opt/conda/bin/python python /opt/conda/bin/python3 2

# install NLP packages *mecab-ko & khai*
WORKDIR /tmp
RUN curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz && \
    tar zxfv mecab-0.996-ko-0.9.2.tar.gz && \
    cd mecab-0.996-ko-0.9.2 && \
    ./configure && \
    make -j$(nproc) && \
    make check && \
    make install

RUN echo "Install mecab-ko-dic" && \
    cd /tmp && \
    ldconfig && \
    curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz && \
    tar -zxvf mecab-ko-dic-2.1.1-20180720.tar.gz && \
    cd mecab-ko-dic-2.1.1-20180720 && \
    ./autogen.sh && \
    ./configure && \
    make -j$(nproc) && \
    sh -c 'echo "dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic" > /usr/local/etc/mecabrc' && \
    make install && \
    cd /tmp && \
    git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git && \
    python3 -m pip install /tmp/mecab-python-0.996

RUN curl -sL https://deb.nodesource.com/setup_16.x | bash - && \
    apt-get update -y && \
    apt-get install -y nodejs

WORKDIR /tmp
<<<<<<< HEAD
RUN git clone -q --branch=v0.3.20 git://github.com/xianyi/OpenBLAS.git && \
=======
RUN git clone -q --branch=v0.3.20 https://github.com/xianyi/OpenBLAS.git && \
>>>>>>> 428b8f2fe85b44c569e7749658a1f56e4b7778b3
    cd OpenBLAS && \
    make DYNAMIC_ARCH=1 NO_AFFINITY=1 NUM_THREADS=48 FC=gfortran && \
    make install && \
    cd /tmp && \
    git clone --recursive https://github.com/bodono/scs-python.git  && \
    cd /tmp/scs-python && \
    python setup.py install --scs --gpu

<<<<<<< HEAD
RUN /opt/conda/bin/conda install opencv ffmpeg spacy 
=======
<<<<<<< HEAD
RUN /opt/conda/bin/conda install -c conda-forge opencv ffmpeg spacy
=======
RUN /opt/conda/bin/conda install -cá…§ conda-forge opencv ffmpeg spacy 
>>>>>>> 428b8f2fe85b44c569e7749658a1f56e4b7778b3
>>>>>>> 6a6593ec74e88d0a437b40681438d777201cafec

WORKDIR /tmp
COPY ./requirements.22.03.txt /tmp/requirements.txt
RUN /opt/conda/bin/python3 -m pip install --no-cache-dir -r requirements.txt && \
    rm -f /tmp/*.whl /tmp/requirements.txt

# install git-lfs
WORKDIR /tmp
RUN curl -sLO https://github.com/git-lfs/git-lfs/releases/download/v3.0.2/git-lfs-linux-amd64-v3.0.2.tar.gz && \
    tar -zxf git-lfs-linux-amd64-v3.0.2.tar.gz && \
    bash install.sh && \
    rm -rf /tmp/*

WORKDIR /tmp
RUN git clone https://github.com/aristocratos/bashtop.git && \
    cd bashtop && \
    make install

RUN curl -fL https://github.com/cdr/code-server/releases/download/v4.0.2/code-server-4.0.2-linux-amd64.tar.gz \
  | tar -C /usr/local/lib -xz && \ 
    mv /usr/local/lib/code-server-4.0.2-linux-amd64 /usr/local/lib/code-server-4.0.2 && \
    ln -s /usr/local/lib/code-server-4.0.2/bin/code-server /usr/local/bin/code-server

# Install Open MPI
RUN mkdir /tmp/openmpi && \
    cd /tmp/openmpi && \
    wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.1.tar.gz && \
    tar zxf openmpi-4.1.1.tar.gz && \
    cd openmpi-4.1.1 && \
    ./configure --enable-orterun-prefix-by-default && \
    make -j $(nproc) all && \
    make install && \
    ldconfig && \
    rm -rf /tmp/openmpi* && \
# Create a wrapper for OpenMPI to allow running as root by default
    mv /usr/local/bin/mpirun /usr/local/bin/mpirun.real && \
    echo '#!/bin/bash' > /usr/local/bin/mpirun && \
    echo 'mpirun.real --allow-run-as-root "$@"' >> /usr/local/bin/mpirun && \
    chmod a+x /usr/local/bin/mpirun && \

# Configure OpenMPI to run good defaults:
    echo "btl_tcp_if_exclude = lo,docker0" >> /usr/local/etc/openmpi-mca-params.conf

# Install Horovod, temporarily using CUDA stubs
<<<<<<< HEAD
RUN cp /usr/local/cuda-11.6/bin/nvcc /bin/nvcc && \
#    ldconfig /usr/local/cuda/targets/x86_64-linux/lib/stubs && \
    HOROVOD_CUDA_HOME=$CONDA_PREFIX HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_GPU_BROADCAST=NCCL HOROVOD_NCCL_LINK=SHARED \
    HOROVOD_WITHOUT_TENSORFLOW=1 HOROVOD_WITH_PYTORCH=1 HOROVOD_WITHOUT_MXNET=1 \
    pip install --no-cache-dir horovod==0.24.1 && \
=======
RUN ldconfig /usr/local/cuda/targets/x86_64-linux/lib/stubs && \
#    cp /usr/local/cuda-11.6/bin/nvcc /bin/nvcc && \
#    ldconfig /usr/local/cuda/targets/x86_64-linux/lib/stubs && \
    HOROVOD_CUDA_HOME=$CONDA_PREFIX HOROVOD_GPU_ALLREDUCE=NCCL HOROVOD_GPU_BROADCAST=NCCL HOROVOD_NCCL_LINK=SHARED \
    HOROVOD_WITHOUT_TENSORFLOW=1 HOROVOD_WITH_PYTORCH=1 HOROVOD_WITHOUT_MXNET=1 \
    pip install --no-cache-dir horovod==0.24.2 && \
>>>>>>> 428b8f2fe85b44c569e7749658a1f56e4b7778b3
    ldconfig

RUN python3 -m pip install --no-cache-dir \
    	mpi4py==3.1.2 \
	    nni==2.5 \
	    mlflow==1.21.0 \
	    scikit-nni==0.2.1

<<<<<<< HEAD
=======
RUN rm /usr/local/bin/node 
>>>>>>> 428b8f2fe85b44c569e7749658a1f56e4b7778b3
RUN jupyter nbextensions_configurator enable && \
    jupyter contrib nbextension install && \
    jupyter nbextension enable --py --sys-prefix widgetsnbextension && \
    jupyter serverextension enable --py jupyterlab --sys-prefix && \
    jupyter labextension install --no-build @jupyter-widgets/jupyterlab-manager && \
    jupyter labextension install --no-build @krassowski/jupyterlab-lsp && \
    jupyter serverextension enable --py jupyter_lsp && \
    jupyter labextension install --no-build @jupyterlab/toc && \
    jupyter nbextension enable execute_time/ExecuteTime && \
    jupyter nbextension enable toc2/main && \
    jupyter lab build

RUN apt autoclean && \
    sed -i 's/source \/usr\/local\/nvm\/nvm.sh//' /etc/bash.bashrc && \
    ln -sf /usr/share/terminfo/x/xterm-color /usr/share/terminfo/x/xterm-256color && \
    rm -rf /var/lib/apt/lists/* && \	
    rm -rf /root/.cache && \
    rm -rf /tmp/*

RUN /opt/conda/bin/python3 -m ipykernel install \
        --prefix=/opt/conda/ \
        --display-name "PyTorch 1.11 (NGC 22.03/Python 3.8 Conda) on Backend.AI" && \
    cat /opt/conda/share/jupyter/kernels/python3/kernel.json

# Backend.AI specifics
COPY ./service-defs /etc/backend.ai/service-defs
COPY ./runner-scripts/bootstrap.sh runner-scripts/setup_multinode.py /opt/container/

LABEL ai.backend.kernelspec="1" \
      ai.backend.envs.corecount="OPENBLAS_NUM_THREADS,OMP_NUM_THREADS,NPROC" \
      ai.backend.features="batch query uid-match user-input" \
      ai.backend.base-distro="ubuntu16.04" \
      ai.backend.accelerators="cuda" \
      ai.backend.resource.min.cpu="1" \
      ai.backend.resource.min.mem="1g" \
      ai.backend.resource.min.cuda.device=1 \
      ai.backend.resource.min.cuda.shares=0 \
      ai.backend.base-distro="ubuntu16.04" \
      ai.backend.runtime-type="python" \
      ai.backend.runtime-path="/opt/conda/bin/python3" \
      ai.backend.service-ports="ipython:pty:3000,jupyter:http:8091,jupyterlab:http:8090,vscode:http:8180,tensorboard:http:6006,mlflow-ui:preopen:5000,nniboard:preopen:8080"

WORKDIR /home/work
